<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>From Models to Decisions: A Practical Mental Model for Applied ML — Gökhan Çetinkaya</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="A conceptual framework for applied machine learning that separates prediction, uncertainty, dynamics, and decisions — and explains why many ML systems fail despite good models." />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <main>
    <header>
      <a href="index.html" class="pill">gokhancetinkaya.ai</a>
      <h1>From Models to Decisions: A Practical Mental Model for Applied ML</h1>
      <p class="muted">
        A synthesis essay on prediction, uncertainty, dynamics, and choice in real-world ML systems.
      </p>
    </header>

    <article class="section post">

      <p>
        Applied machine learning has matured enough that most teams no longer fail at the basics.
      </p>

      <p>
        Data pipelines exist. Models train reliably. Validation is understood. Metrics are tracked.
      </p>

      <p>
        And yet, a strange pattern persists: systems that are technically correct still produce outcomes that feel misaligned with business reality.
      </p>

      <p>
        Not catastrophically wrong. Just… persistently disappointing.
      </p>

      <p>
        This is not a tooling problem. It is a framing problem.
      </p>

      <h2>The missing distinction</h2>

      <p>
        In many ML projects, three very different questions get quietly blended into one:
      </p>

      <ol>
        <li><em>What can we predict?</em></li>
        <li><em>How confident are we?</em></li>
        <li><em>What should we do about it?</em></li>
      </ol>

      <p>
        These questions sound adjacent, but they live at different conceptual levels. Treating them as interchangeable is the root cause of much confusion in applied ML.
      </p>

      <p>
        The failure is not mathematical. It is architectural — in how we think.
      </p>

      <h2>A four-layer mental model</h2>

      <p>
        A useful way to regain clarity is to separate ML-powered systems into <strong>four conceptual layers</strong>.
      </p>

      <p>
        Not software layers. Thinking layers.
      </p>

<div style="display: flex; justify-content: center;">
<pre style="text-align: left; margin: 1.2rem 0;">
[ Decisions ]
      ↑
[ Dynamics / Simulation ]
      ↑
[ Uncertainty ]
      ↑
[ Prediction ]
</pre>
</div>

      <p>
        Each layer answers a fundamentally different question.
      </p>

      <h3>Prediction: what patterns exist?</h3>

      <p>
        This is the domain of machine learning.
      </p>

      <p>
        Forecasts, classifications, scores, rankings. We ask: <em>given historical data, what tends to happen?</em>
      </p>

      <p>
        This layer is descriptive. Powerful. Necessary.
      </p>

      <p>
        But by itself, it is silent about consequences.
      </p>

      <h3>Uncertainty: how wrong could we be?</h3>

      <p>
        Here we admit epistemic limits.
      </p>

      <p>
        Distributions instead of points. Intervals instead of single numbers. Ensembles, Bayesian posteriors, empirical quantiles.
      </p>

      <p>
        This layer does not make decisions safer. It makes ignorance visible.
      </p>

      <p>
        Which is uncomfortable — but essential.
      </p>

      <h3>Dynamics: what happens when we act?</h3>

      <p>
        Now time enters the picture.
      </p>

      <p>
        Actions change the system. Systems react to themselves. Delays, accumulation, thresholds, feedback loops appear.
      </p>

      <p>
        This is the layer where simulation belongs.
      </p>

      <p>
        Not to predict the future precisely, but to explore <strong>plausible trajectories</strong> once decisions interact with uncertainty over time.
      </p>

      <p>
        This layer is where many “good models” quietly fail.
      </p>

      <h3>Decisions: what trade-offs are we willing to make?</h3>

      <p>
        Only at this layer do questions become genuinely business-relevant.
      </p>

      <p>
        Costs, constraints, incentives, service levels, risk tolerance.
      </p>

      <p>
        Not <em>what is accurate</em>, but <em>what is acceptable</em>.
      </p>

      <p>
        Decisions are policies, not predictions. They embed values — explicitly or not.
      </p>

      <h2>Why this separation matters</h2>

      <p>
        Most applied ML systems collapse these layers.
      </p>

      <p>
        Prediction stands in for decision. Accuracy stands in for value. Automation stands in for judgment.
      </p>

      <p>
        This works until it doesn’t.
      </p>

      <p>
        When outcomes drift, teams often react by improving the prediction layer — adding features, retraining models, tightening metrics.
      </p>

      <p>
        Sometimes that helps. Often it doesn’t, because the mismatch lives higher up.
      </p>

      <h2>A different way to measure progress</h2>

      <p>
        Once you adopt this layered view, progress looks different.
      </p>

      <p>
        You stop asking: “Is the model good enough?”
      </p>

      <p>
        And start asking:
      </p>

      <ul>
        <li>Is this decision robust across plausible futures?</li>
        <li>Which assumptions actually drive outcomes?</li>
        <li>Where does risk concentrate, and who bears it?</li>
      </ul>

      <p>
        This is a shift from model-centric thinking to <strong>system-centric thinking</strong>.
      </p>

      <h2>A reframing for experienced practitioners</h2>

      <p>
        For ML practitioners, this can be a subtle but profound shift.
      </p>

      <p>
        Your value is not proportional to how complex your model is. It is proportional to how much <strong>avoidable regret</strong> you remove from the system.
      </p>

      <p>
        Regret doesn’t come from being a few percent off on a metric. It comes from discovering — too late — that a different decision policy would have behaved better under stress.
      </p>

      <p>
        That is not a modeling problem. It is a design problem.
      </p>

      <h2>How the pieces complement each other</h2>

      <p>
        Seen through this lens, familiar methods fall into place:
      </p>

      <ul>
        <li><strong>Machine learning</strong> extracts signal.</li>
        <li><strong>Bayesian thinking</strong> disciplines uncertainty.</li>
        <li><strong>Simulation</strong> explores consequences.</li>
        <li><strong>Optimization</strong> formalizes choices.</li>
      </ul>

      <p>
        None is sufficient alone. Each becomes dangerous when mistaken for the whole.
      </p>

      <h2>Closing thoughts</h2>

      <p>
        This is not a call to abandon ML rigor.
      </p>

      <p>
        It is a call to place it inside a broader decision framework.
      </p>

      <p>
        Strong models matter. But models do not make decisions — systems do.
      </p>

      <p>
        If applied ML is to mature beyond impressive demos and fragile automations, it needs clearer boundaries between prediction, uncertainty, dynamics, and choice.
      </p>

      <p>
        That separation is not academic. It is the difference between systems that merely work — and systems that endure.
      </p>

    </article>

    <footer>
      <p><a href="index.html">Home</a></p>
      <p class="muted">
        This site is intentionally simple: no tracking, no pop-ups.
        Just a place to think clearly about ML, AI, and decision-making.
      </p>
    </footer>
  </main>
</body>
</html>