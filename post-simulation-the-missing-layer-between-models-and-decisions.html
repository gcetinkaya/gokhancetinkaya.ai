

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Simulation: The Missing Layer Between Models and Decisions — Gökhan Çetinkaya</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Why simulation is often the missing layer between prediction, inference, and operational decisions: feedback loops, delays, and tail risk become visible only when you run the system over time." />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <main>
    <header>
      <a href="index.html" class="pill">gokhancetinkaya.ai</a>
      <h1>Simulation: The Missing Layer Between Models and Decisions</h1>
      <p class="muted">A practical essay on dynamics, feedback, and why “good models” still produce disappointing outcomes.</p>
    </header>

    <article class="section post">
      <p>
        Most applied ML systems do not fail dramatically.
      </p>

      <p>
        They fail quietly.
      </p>

      <p>
        At first, everything looks fine. Forecasts are stable. KPIs move in the right direction. Dashboards reassure everyone that the system is “working.”
      </p>

      <p>
        Then, slowly, side effects appear.
      </p>

      <p>
        Service quality erodes in places no one was watching. Costs creep up in ways that don’t trace cleanly back to a single decision. Teams start adding manual overrides, buffers, exceptions — not because the model is wrong, but because reality keeps pushing back.
      </p>

      <p>
        When this happens, the issue is rarely predictive accuracy. It’s that the system is reacting to itself over time.
      </p>

      <p>
        This is where simulation becomes indispensable — not as an academic exercise, but as a way to reason about consequences before they compound.
      </p>

      <h2>Decisions are embedded in systems</h2>

      <p>
        A forecast is a number. A decision is an intervention.
      </p>

      <p>
        Between the two sits a system with memory, constraints, and feedback.
      </p>

      <p>
        If you have worked on any operational problem, you have seen this firsthand:
      </p>

      <ul>
        <li>Decisions are executed on a <strong>cadence</strong> (weekly planning, monthly reviews, quarterly budgets).</li>
        <li>Capacity is <strong>lumpy</strong>, not continuous (people, machines, space).</li>
        <li>Actions today shape the data tomorrow.</li>
        <li>Small delays accumulate; rare events dominate outcomes.</li>
      </ul>

      <p>
        None of this fits neatly into a single predictive model.
      </p>

      <p>
        Even probabilistic models struggle once you introduce batching, delayed effects, human responses, and policies that interact over time.
      </p>

      <p>
        Yet this is precisely the environment in which real decisions are made.
      </p>

      <p>
        Simulation exists to operate in that gap.
      </p>

      <h2>What simulation actually does</h2>

      <p>
        Simulation is often misunderstood as “just Monte Carlo” or “a fancy visualization.” It is neither.
      </p>

      <p>
        At its core, simulation is structured counterfactual reasoning:
      </p>

      <blockquote>
        <p><em>If we follow this policy, in a world with these uncertainties and constraints, what tends to happen over time?</em></p>
      </blockquote>

      <p>
        It does not ask “What is the correct prediction?” It asks:
      </p>

      <ul>
        <li>What behavior does this decision induce?</li>
        <li>Where does risk accumulate?</li>
        <li>Which assumptions matter, and which don’t?</li>
      </ul>

      <p>
        Simulation does not give you <em>the</em> answer. It shows you the <strong>shape of the consequences</strong>.
      </p>

      <h2>Why simulation succeeds where better models don’t</h2>

      <p>
        A common instinct in applied ML is to respond to disappointing outcomes by improving the model: more features, more data, better architecture, tighter loss functions.
      </p>

      <p>
        Sometimes this helps.
      </p>

      <p>
        Often, it doesn’t — because the failure mode is structural, not statistical.
      </p>

      <p>
        The decision problem is rarely “minimize error.” It is “minimize regret”: the cost of realizing, weeks later, that a different policy would have avoided churn, shortfall, and an expensive recovery.
      </p>

      <p>
        Simulation helps because it can hold several realities at once: uncertainty, constraints, policy interactions, and tail events.
      </p>

      <p>
        And it does this without requiring us to pretend that the system can be fully captured in one elegant equation.
      </p>

      <p>
        A useful way to think about it:
      </p>

      <blockquote>
        <p><em>Simulation lets you be wrong in many small ways, instead of wrong in one big structural way.</em></p>
      </blockquote>

      <p>
        That is not a concession. It is rigor of a different kind.
      </p>

      <h2>A minimal simulation you can inspect</h2>

      <p>
        In the companion notebook, the setup is deliberately boring:
      </p>

      <ul>
        <li>Weekly demand is random, with a fixed mean.</li>
        <li>Halfway through the horizon, demand variance increases (a crude stand-in for regime change).</li>
        <li>Capacity equals headcount times per-person throughput.</li>
        <li>Hiring takes time (a fixed delay pipeline).</li>
        <li>Burnout accumulates when utilization stays above a healthy threshold.</li>
        <li>Attrition rises nonlinearly with burnout.</li>
      </ul>

      <p>
        The only difference between the two policies is the utilization target:
      </p>

      <ul>
        <li><strong>Policy A</strong> aims for ~85% utilization (lean, “efficient”).</li>
        <li><strong>Policy B</strong> aims for ~75% utilization (slack, “wasteful”).</li>
      </ul>

      <p>
        Both policies see the same demand process. Neither has a “better forecast.”
        Yet the distributions of outcomes can diverge sharply once feedback and delays are allowed to do their thing.
      </p>

      <p>
        A single trajectory is illustrative, but the point shows up most clearly when you run many simulations:
        the median can look acceptable while the tails become painful.
      </p>

      <p class="muted">
        Notebook:
        <a href="https://github.com/gcetinkaya/gokhancetinkaya.ai/blob/main/examples/staffing_burnout_sim/examples_post4_staffing_burnout_sim.ipynb" target="_blank" rel="noopener">
          staffing + burnout feedback simulation (Jupyter notebook)</a>.
        It is intentionally simple and focuses on system dynamics (feedback, delays, tails), not on finding an “optimal” policy.
      </p>

      <h2>Simulation as the connective tissue</h2>

      <p>
        Seen this way, simulation naturally complements other methods:
      </p>

      <ul>
        <li><strong>Machine learning</strong> learns relationships from data.</li>
        <li><strong>Bayesian inference</strong> expresses uncertainty about unknowns.</li>
        <li><strong>Optimization</strong> selects actions under constraints.</li>
        <li><strong>Simulation</strong> reveals what those actions do once uncertainty, time, and feedback are allowed to exist.</li>
      </ul>

      <p>
        This is where uncertainty becomes operational.
      </p>

      <p>
        You can feed simulation with point forecasts, quantiles, ensembles, or posterior samples.
        What matters is not mathematical purity, but whether decisions are evaluated in an environment that resembles reality.
      </p>

      <h2>Why simulation remains underused</h2>

      <p>
        If simulation is so powerful, why isn’t it everywhere?
      </p>

      <p>
        Because it is inconvenient.
      </p>

      <ul>
        <li>It does not produce a single “best” number.</li>
        <li>It forces policies to be explicit.</li>
        <li>It exposes trade-offs that organizations would rather postpone.</li>
        <li>It often reveals that the biggest lever is procedural, not algorithmic.</li>
      </ul>

      <p>
        Simulation is not just a technical tool. It is an organizational mirror.
      </p>

      <h2>Closing thoughts</h2>

      <p>
        Models help us understand. Optimization helps us choose. Simulation helps us live with the consequences.
      </p>

      <p>
        If your system looks good on a dashboard but surprises you in production, the problem is rarely that your model was insufficiently sophisticated.
      </p>

      <p>
        More often, it’s that you never gave your decisions a chance to fail safely — on your terms — before the real world did it for you.
      </p>
    </article>

    <footer>
      <p><a href="index.html">Home</a></p>
      <p class="muted">
        This site is intentionally simple: no tracking, no pop-ups.
        Just a place to think clearly about ML, AI, and decision-making.
      </p>
    </footer>
  </main>
</body>
</html>